{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976d31d6",
   "metadata": {},
   "source": [
    "# Coleta e Tratamento de Variáveis Econômicas e Climáticas\n",
    "\n",
    "**Projeto:** \"Modelagem Preditiva de Risco de Crédito sob *Stress* Macroeconômico: uma abordade comparativa en *Ensemble Trees* e Regressão Regularizada\"<br>\n",
    "**Autor:** Pedro Rogério Pereira Júnior (Pereira Júnior, P. R.)<br>\n",
    "**Data:** Dezembro/2025<br>\n",
    "\n",
    "## 1. Objetivo\n",
    "Este notebook realiza o processo de **ETL (Extract, Transform, Load)** para consolidar uma base de dados analítica. Os dados são coletados de fontes oficiais públicas:\n",
    "* **Banco Central do Brasil (BCB/SGS):** Inadimplência, Selic, IPCA, Dólar, etc.\n",
    "* **IBGE (SIDRA):** Dados de emprego (PNAD) e rendimento.\n",
    "* **INMET:** Dados climáticos históricos (precipitação, temperatura, etc.).\n",
    "\n",
    "O objetivo final é gerar arquivos `.csv` padronizados na pasta `data/raw` para a modelagem subsequente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3cb903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório de saída configurado: C:\\Users\\pedro\\projeto_inadimplencia\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Configurações Iniciais e Bibliotecas ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandera as pa\n",
    "from pandera import Column, DataFrameSchema, Check\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import io\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "\n",
    "# Ignorar warnings de conexões seguras para manter o log limpo\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# CONFIGURAÇÃO DE DIRETÓRIOS\n",
    "BASE_PATH = Path(\"../data/raw\")\n",
    "BASE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Diretório de saída configurado: {BASE_PATH.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f7de6",
   "metadata": {},
   "source": [
    "## 2. Coleta de Dados do Banco Central (SGS)\n",
    "Utilizamos a API do Sistema Gerenciador de Séries Temporais (SGS) para capturar indicadores macroeconômicos e de crédito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa7f627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando coleta do Banco Central...\n",
      "Dados do BCB coletados e consolidados.\n"
     ]
    }
   ],
   "source": [
    "# --- Função de Coleta BCB ---\n",
    "def coleta_bcb(codigo: int, nome: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Coleta uma série temporal do Banco Central (SGS) e retorna um DataFrame formatado.\n",
    "    \n",
    "    Args:\n",
    "        codigo (int): Código da série no SGS.\n",
    "        nome (str): Nome que a coluna de valor receberá.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.bcb.gov.br/dados/serie/bcdata.sgs.{codigo}/dados?formato=json&dataInicial=01/01/2015&dataFinal=31/12/2024\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status() # Alerta se houver erro na requisição (404, 500)\n",
    "        dados = pd.DataFrame(r.json())\n",
    "        dados[\"data\"] = pd.to_datetime(dados[\"data\"], dayfirst=True)\n",
    "        dados[\"valor\"] = pd.to_numeric(dados[\"valor\"], errors=\"coerce\")\n",
    "        dados.rename(columns={\"valor\": nome}, inplace=True)\n",
    "        return dados\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao coletar série {codigo}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"Iniciando coleta do Banco Central...\")\n",
    "\n",
    "# 1. Inadimplência\n",
    "inad_pj_tot = coleta_bcb(21083, \"inad_pj_tot\")\n",
    "inad_pf_tot = coleta_bcb(21084, \"inad_pf_tot\")\n",
    "inad_rd_pf_cr_rur_tot = coleta_bcb(21148, \"inad_rd_pf_cr_rur_tot\")\n",
    "inad_rd_pj_cr_rur_tot = coleta_bcb(21136, \"inad_rd_pj_cr_rur_tot\")\n",
    "\n",
    "df_inadimplencia = reduce(lambda left, right: pd.merge(left, right, on=\"data\", how=\"outer\"), \n",
    "                          [inad_pj_tot, inad_pf_tot, inad_rd_pf_cr_rur_tot, inad_rd_pj_cr_rur_tot])\n",
    "df_inadimplencia = df_inadimplencia.sort_values(\"data\").reset_index(drop=True)\n",
    "df_inadimplencia[\"data_str\"] = df_inadimplencia[\"data\"].dt.strftime(\"%m/%Y\") # Mantendo coluna original data para ordenação\n",
    "\n",
    "# 2. Variáveis Econômicas Diversas\n",
    "selic = coleta_bcb(11, \"selic\")\n",
    "ipca = coleta_bcb(4449, \"ipca\")\n",
    "cdi = coleta_bcb(12, \"cdi\")\n",
    "spread_pf = coleta_bcb(20785, \"spread_pf\")\n",
    "spread_pj = coleta_bcb(20784, \"spread_pj\")\n",
    "spread_tot = coleta_bcb(20783, \"spread_tot\")\n",
    "dolar = coleta_bcb(1, \"dolar_ptax\")\n",
    "icms_petroleo = coleta_bcb(7694, \"icms_petroleo\")\n",
    "soja_triturada = coleta_bcb(2971, \"soja_triturada_expt\")\n",
    "soja_residuo = coleta_bcb(2959, \"soja_residuo_expt\")\n",
    "soja_oleo = coleta_bcb(2959, \"soja_oleo_expt\")\n",
    "milho_grao = coleta_bcb(20213, \"milho_grao_expt\")\n",
    "\n",
    "# Consolidar Mensais\n",
    "lista_mensal = [ipca, spread_pf, spread_pj, spread_tot, icms_petroleo, soja_triturada, soja_residuo, soja_oleo, milho_grao]\n",
    "df_econ_mensal = reduce(lambda left, right: pd.merge(left, right, on=\"data\", how=\"outer\"), lista_mensal)\n",
    "\n",
    "# Consolidar Diárias (Transformando em média mensal)\n",
    "lista_diaria = [selic, cdi, dolar]\n",
    "df_econ_diaria_raw = reduce(lambda left, right: pd.merge(left, right, on=\"data\", how=\"outer\"), lista_diaria)\n",
    "df_econ_diaria = df_econ_diaria_raw.set_index(\"data\").resample(\"ME\").mean().reset_index()\n",
    "\n",
    "# Merge Final Economia\n",
    "df_economico = pd.merge(df_econ_mensal, df_econ_diaria, on=\"data\", how=\"outer\").sort_values(\"data\").reset_index(drop=True)\n",
    "df_economico[\"data_str\"] = df_economico[\"data\"].dt.strftime(\"%m/%Y\")\n",
    "\n",
    "print(\"Dados do BCB coletados e consolidados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e3b5c",
   "metadata": {},
   "source": [
    "## 3. Coleta de Dados do IBGE (SIDRA)\n",
    "Extração de dados da PNAD Contínua (Emprego) e Rendimento médio.\n",
    "* **Tratamento:** As datas vêm no formato \"trimestre móvel\" ou ano, exigindo conversão para frequência mensal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48f20d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando coleta do IBGE...\n",
      "Dados IBGE coletados.\n"
     ]
    }
   ],
   "source": [
    "# --- Coleta IBGE ---\n",
    "print(\"Iniciando coleta do IBGE...\")\n",
    "\n",
    "# A. Renda\n",
    "url_renda = \"https://apisidra.ibge.gov.br/values/t/7437/n1/all/v/10750/p/last%2010/c11308/49204\"\n",
    "df_renda = pd.DataFrame(requests.get(url_renda).json()).iloc[1:]\n",
    "df_renda = df_renda[[\"D3C\", \"V\"]].rename(columns={\"D3C\": \"data\", \"V\": \"rendimento_medio_mensal_reais\"})\n",
    "df_renda[\"rendimento_medio_mensal_reais\"] = pd.to_numeric(df_renda[\"rendimento_medio_mensal_reais\"], errors=\"coerce\")\n",
    "df_renda[\"data\"] = pd.to_datetime(df_renda[\"data\"], format=\"%Y\")\n",
    "# Resample anual para mensal (preenchimento forward)\n",
    "df_renda_mensal = df_renda.set_index(\"data\").resample(\"ME\").ffill().reset_index()\n",
    "\n",
    "# B. Trabalho (PNAD)\n",
    "url_trabalho = \"https://apisidra.ibge.gov.br/values/t/4093/n1/all/v/1641,4088,4090,4092,4094,4096,4099,12466/p/all\"\n",
    "df_trabalho = pd.DataFrame(requests.get(url_trabalho).json()).iloc[1:]\n",
    "df_trabalho = df_trabalho[[\"D2C\", \"D3C\", \"V\"]].rename(columns={\"D2C\": \"variavel\", \"D3C\": \"data\", \"V\": \"valor\"})\n",
    "\n",
    "def clean_ibge_trimestre(date_str):\n",
    "    if len(str(date_str)) == 6 and str(date_str).isdigit():\n",
    "        ano = int(str(date_str)[:4])\n",
    "        trim = int(str(date_str)[4:])\n",
    "        mes_map = {1: 1, 2: 4, 3: 7, 4: 10}\n",
    "        return pd.Timestamp(year=ano, month=mes_map.get(trim, 1), day=1)\n",
    "    return pd.NaT\n",
    "\n",
    "df_trabalho[\"data\"] = df_trabalho[\"data\"].apply(clean_ibge_trimestre)\n",
    "df_trabalho[\"valor\"] = pd.to_numeric(df_trabalho[\"valor\"], errors=\"coerce\")\n",
    "df_trabalho = df_trabalho.dropna(subset=[\"data\"]).pivot_table(index=\"data\", columns=\"variavel\", values=\"valor\").reset_index()\n",
    "\n",
    "# Renomear colunas\n",
    "cols_map = {\n",
    "    \"1641\": \"pessoas_14mais_mil\", \"4088\": \"p14m_forca_trabalho_mil\", \n",
    "    \"4090\": \"p14m_ocupadas_mil\", \"4092\": \"p14m_desocupadas_mil\",\n",
    "    \"4094\": \"p14m_fora_forca_trabalho_mil\", \"4096\": \"tx_foca_trabalho_p14m_pct\",\n",
    "    \"4099\": \"tx_desocupacao_p14m_pct\", \"12466\": \"tx_informalidade_p14m_ocupadas_pct\"\n",
    "}\n",
    "df_trabalho.rename(columns=cols_map, inplace=True)\n",
    "df_trabalho_mensal = df_trabalho.set_index(\"data\").resample(\"ME\").ffill().reset_index()\n",
    "\n",
    "# Consolidar IBGE\n",
    "df_ibge = pd.merge(df_renda_mensal, df_trabalho_mensal, on=\"data\", how=\"outer\").sort_values(\"data\")\n",
    "df_ibge[\"data_str\"] = df_ibge[\"data\"].dt.strftime(\"%m/%Y\")\n",
    "\n",
    "print(\"Dados IBGE coletados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c3ad29",
   "metadata": {},
   "source": [
    "## 4. Processamento de Dados Climáticos (INMET)\n",
    "Os dados climáticos são volumosos e fornecidos em arquivos `.zip`.\n",
    "* **Estratégia:** Iterar sobre os arquivos CSV dentro do zip sem extraí-los fisicamente para o disco (leitura em memória), otimizando o processamento.\n",
    "* **Tratamento:** Identificação dinâmica do início da tabela de dados e cálculo de médias mensais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d52f5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando dados climáticos de: ..\\data\\raw\\INMET.zip\n",
      "Total de estações/arquivos encontrados: 322\n",
      "Processando 0/322...\n",
      "Processando 50/322...\n",
      "Processando 100/322...\n",
      "Processando 150/322...\n",
      "Processando 200/322...\n",
      "Processando 250/322...\n",
      "Processando 300/322...\n",
      "Dados Climáticos processados.\n"
     ]
    }
   ],
   "source": [
    "# --- Coleta INMET ---\n",
    "zip_path = BASE_PATH / \"INMET.zip\" \n",
    "\n",
    "print(f\"Processando dados climáticos de: {zip_path}\")\n",
    "\n",
    "if not zip_path.exists():\n",
    "    print(f\"ALERTA: Arquivo {zip_path} não encontrado. Pule esta etapa se não tiver os dados locais.\")\n",
    "else:\n",
    "    dfs_inmet = []\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        csvs = [f for f in z.namelist() if f.lower().endswith(\".csv\")]\n",
    "        print(f\"Total de estações/arquivos encontrados: {len(csvs)}\")\n",
    "        \n",
    "        for i, arquivo in enumerate(csvs):\n",
    "            if i % 50 == 0: print(f\"Processando {i}/{len(csvs)}...\") # Log menos verboso\n",
    "            \n",
    "            with z.open(arquivo) as f:\n",
    "                content = f.read().decode(\"latin1\")\n",
    "                lines = content.splitlines()\n",
    "                start_row = next((idx for idx, l in enumerate(lines) if \"Data Medicao\" in l), None)\n",
    "                \n",
    "                if start_row is not None:\n",
    "                    tabela = \"\\n\".join(lines[start_row:])\n",
    "                    df_temp = pd.read_csv(io.StringIO(tabela), sep=\";\", decimal=\",\", encoding=\"latin1\")\n",
    "                    dfs_inmet.append(df_temp)\n",
    "\n",
    "    # Concatenação e Limpeza\n",
    "    if dfs_inmet:\n",
    "        df_inmet_full = pd.concat(dfs_inmet, ignore_index=True)\n",
    "        \n",
    "        map_cols = {\n",
    "            \"Data Medicao\": \"data\", \"INSOLACAO TOTAL, MENSAL(h)\": \"insolacao_total_h\",\n",
    "            \"PRECIPITACAO TOTAL, MENSAL(mm)\": \"precipitacao_total_mm\",\n",
    "            \"TEMPERATURA MAXIMA MEDIA, MENSAL(Â°C)\": \"temp_max_media_c\",\n",
    "            \"UMIDADE RELATIVA DO AR, MEDIA MENSAL(%)\": \"umidade_media_pct\"\n",
    "        }\n",
    "        \n",
    "        df_inmet_clean = df_inmet_full.rename(columns=map_cols)[list(map_cols.values())]\n",
    "        df_inmet_clean[\"data\"] = pd.to_datetime(df_inmet_clean[\"data\"], errors=\"coerce\")\n",
    "        \n",
    "        # Agrupamento Mensal (Média de todas as estações do Brasil para simplificação macro)\n",
    "        df_inmet_final = df_inmet_clean.groupby(\"data\").mean().reset_index().round(2)\n",
    "        df_inmet_final[\"data_str\"] = df_inmet_final[\"data\"].dt.strftime(\"%m/%Y\")\n",
    "        \n",
    "        print(\"Dados Climáticos processados.\")\n",
    "    else:\n",
    "        df_inmet_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f495f15",
   "metadata": {},
   "source": [
    "## 5. Engenharia de Variáveis: Eventos Políticos e Calendário\n",
    "Criação de variáveis *dummy* (binárias) para marcar períodos históricos relevantes que podem explicar choques na inadimplência, como:\n",
    "* Pandemia de COVID-19.\n",
    "* Greve dos Caminhoneiros.\n",
    "* Trocas de mandatos presidenciais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c19ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validação de Eventos: Sucesso\n"
     ]
    }
   ],
   "source": [
    "# --- Dataset de Eventos ---\n",
    "def criar_eventos(start=\"2015-01-01\", end=\"2024-12-31\"):\n",
    "    dates = pd.date_range(start=start, end=end, freq=\"MS\")\n",
    "    df = pd.DataFrame({\"data\": dates})\n",
    "    \n",
    "    # Flags Iniciais\n",
    "    df[\"flag_pandemia\"] = 0\n",
    "    df[\"flag_greve_caminhoneiros\"] = 0\n",
    "    df[\"presidente\"] = \"Indefinido\"\n",
    "    \n",
    "    # Regras de Negócio\n",
    "    df.loc[(df[\"data\"] >= \"2020-03-01\") & (df[\"data\"] <= \"2022-05-01\"), \"flag_pandemia\"] = 1\n",
    "    df.loc[df[\"data\"] == \"2018-05-01\", \"flag_greve_caminhoneiros\"] = 1\n",
    "    \n",
    "    # Presidentes\n",
    "    df.loc[df[\"data\"] < \"2016-09-01\", \"presidente\"] = \"Dilma\"\n",
    "    df.loc[(df[\"data\"] >= \"2016-09-01\") & (df[\"data\"] < \"2019-01-01\"), \"presidente\"] = \"Temer\"\n",
    "    df.loc[(df[\"data\"] >= \"2019-01-01\") & (df[\"data\"] < \"2023-01-01\"), \"presidente\"] = \"Bolsonaro\"\n",
    "    df.loc[df[\"data\"] >= \"2023-01-01\", \"presidente\"] = \"Lula\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_eventos = criar_eventos()\n",
    "\n",
    "# Validação Pandera\n",
    "schema_eventos = DataFrameSchema({\n",
    "    \"data\": Column(pd.Timestamp),\n",
    "    \"presidente\": Column(str, checks=pa.Check.isin([\"Dilma\", \"Temer\", \"Bolsonaro\", \"Lula\", \"Indefinido\"])),\n",
    "    \"flag_pandemia\": Column(int, checks=pa.Check.isin([0, 1])),\n",
    "}, strict=False)\n",
    "\n",
    "try:\n",
    "    schema_eventos.validate(df_eventos)\n",
    "    print(\"✅ Validação de Eventos: Sucesso\")\n",
    "    df_eventos[\"data_str\"] = df_eventos[\"data\"].dt.strftime(\"%m/%Y\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro na validação: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029201b",
   "metadata": {},
   "source": [
    "## 6. Exportação dos Dados\n",
    "Salvando os DataFrames processados na pasta `data/raw` para uso nos próximos notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "269ff3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo: ..\\data\\raw\\df_inadimplencia.csv | Shape: (120, 6)\n",
      "Salvo: ..\\data\\raw\\df_economico.csv | Shape: (240, 14)\n",
      "Salvo: ..\\data\\raw\\df_ibge.csv | Shape: (163, 11)\n",
      "Salvo: ..\\data\\raw\\df_inmet.csv | Shape: (144, 6)\n",
      "Salvo: ..\\data\\raw\\df_eventos_politicos.csv | Shape: (120, 5)\n"
     ]
    }
   ],
   "source": [
    "dfs_to_save = {\n",
    "    \"inadimplencia\": df_inadimplencia,\n",
    "    \"economico\": df_economico,\n",
    "    \"ibge\": df_ibge,\n",
    "    \"inmet\": df_inmet_final,\n",
    "    \"eventos_politicos\": df_eventos\n",
    "}\n",
    "\n",
    "for nome, df in dfs_to_save.items():\n",
    "    if not df.empty:\n",
    "        # Removemos a coluna auxiliar 'data' (timestamp) se quisermos apenas a string, \n",
    "        # ou mantemos ambas. Aqui vou salvar limpo, mas mantendo data datetime para sorting se necessario no load\n",
    "        path = BASE_PATH / f\"df_{nome}.csv\"\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"Salvo: {path} | Shape: {df.shape}\")\n",
    "    else:\n",
    "        print(f\"Ignorado (Vazio): {nome}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
